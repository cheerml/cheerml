<?xml version="1.0" encoding="UTF-8" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
        <title>Cheer ML</title>
        <description>Cheer ML - Jun Lu, Yixuan Hu</description>
        <link></link>
        <atom:link href="" rel="self" type="application/rss+xml" />
        <lastBuildDate>Thu, 12 Oct 2017 18:44:12 +0200</lastBuildDate>
        <pubDate>Thu, 12 Oct 2017 18:44:12 +0200</pubDate>
        <ttl>60</ttl>


        <item>
                <title>Dimensionality Reduction via JL Lemma and Random Projection</title>
                <description>&lt;p&gt;Nowadays, dimensionality is a serious problem of data analysis as the huge data we experience today results in very sparse sets and very high dimensions. Although, data scientists have long used tools such as principal component analysis (PCA) and independent component analysis (ICA) to project the high-dimensional data onto a subspace, but all those techniques reply on the computation of the eigenvectors of a $n \times n$ matrix, a very expensive operation (e.g., spectral decomposition) for high dimension $n$. Moreover, even though eigenspace has many important properties, it does not lead good approximations for many useful measures such as vector norms.&lt;/p&gt;

&lt;p&gt;In 1984, two mathematicians introduced and proved the following lemma.&lt;/p&gt;

&lt;h2 id=&quot;johnson-lindenstrauss-lemma&quot;&gt;Johnson-Lindenstrauss lemma&lt;/h2&gt;
&lt;p&gt;For any $\epsilon \in (0,\frac{1}{2})$, $\forall x_1, x_2, \dots, x_d \in R^{n}$, there exists a matrix $M \in R^{m \times n}$ with $m = O(\frac{1}{\epsilon^2} \log{d})$ such that $\forall 1 \leq i,j \leq d$, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(1-\epsilon)||x_i - x_j||_2 \leq ||Mx_i - Mx_j||_2 \leq (1+\epsilon)||x_i - x_j||_2&lt;/script&gt;

&lt;p&gt;Remark: This lemma states that for any pair vector $x_i, x_j$ in $d$ dimension, there exist a sketch matrix $M$ which maps $R^n \rightarrow R^m$ and the Euclidean distance is preserved within $\epsilon$ factor. The result dimension does not have any relationship to origin dimension $n$ (only relates to the number of vector pairs $d$).&lt;/p&gt;

&lt;p&gt;During a long time, no one can figure out how to get this sketch matrix.&lt;/p&gt;

&lt;h2 id=&quot;random-projection&quot;&gt;Random Projection&lt;/h2&gt;
&lt;p&gt;Until 2003, some researches point out that this sketch matrix can be created using Gaussian distribution.&lt;/p&gt;

&lt;p&gt;Consider the following matrix $A \in R^{m \times n}$, where $A_{ij} \sim \mathcal{N}(0,1)$ and all $A_{ij}$ are independent. We claim that this matrix satisfies the statement of JL lemma.&lt;/p&gt;

&lt;p&gt;Proof. It is obvious that sketch has an additional property, 
$\forall i, (Ax)_i = \sum_{j=1}^{n} A_{ij} x_j \sim \mathcal{N}(0, ||x||_2^2)$. In other word, Gaussian distribution is 2-stable distribution. Then we can obtain $||Ax||_2^2 = \sum_{i=1}^{m} y_i^2$, where $y_i \sim \mathcal{N}(0, ||x||_2^2)$. That is to say, $||Ax||_2^2$ follows a $\chi^2$ (chi-squared) distribution with degrees of freedom $m$. For tail bound of $\chi^2$ distribution, we can get&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
P(||Ax||_2^2 - m||X||_2^2| &gt; \epsilon m||X||_2^2) &lt; \exp(-C \epsilon^2 m) %]]&gt;&lt;/script&gt;
for a constant $C &amp;gt; 0$.&lt;/p&gt;

&lt;p&gt;Fix two index $i, j$, and let $y^{ij} = x_i - x_j$ and $M = \frac{1}{\sqrt{m}} A$, and set $m = \frac{4}{C \epsilon^2} \log{n}$ to get&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
P(|||M y^{ij}||_2^2 - ||y^{ij}||_2^2| &gt; \epsilon ||y^{ij}||_2^2) &lt; \exp(-C \epsilon^2 m) = \frac{1}{n^4} %]]&gt;&lt;/script&gt;

&lt;p&gt;Take the union bound to obtain,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
P(\forall i \neq j, |||M y^{ij}||_2^2 - ||y^{ij}||_2^2| &gt; \epsilon ||y^{ij}||_2^2 ) &lt; \sum_{i \neq j} P(|||M y^{ij}||_2^2 - ||y^{ij}||_2^2| &lt; \epsilon ||y^{ij}||_2^2 ) &lt; {n \choose 2} n^4 &lt; \frac{1}{n^2} %]]&gt;&lt;/script&gt;

&lt;p&gt;which is same as the guarantee in Johnson-Lindenstrauss lemma.&lt;/p&gt;

&lt;h2 id=&quot;application&quot;&gt;Application&lt;/h2&gt;
&lt;p&gt;In this or other forms, the JL lemma has been used for a large variety of computational tasks, especially in streaming algorithm, such as&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.stat.berkeley.edu/~mmahoney/f13-stat260-cs294/Lectures/lecture19.pdf&quot;&gt;Computing a low-rank approximation to the original matrix A.&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://web.stanford.edu/class/cs369g/files/lectures/lec16.pdf&quot;&gt;Finding nearest neighbors in high-dimensional space.&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.cs.yale.edu/homes/spielman/561/lect17-15.pdf&quot;&gt;Simplify the calculation of the effective resistance in Graph Spectral Sparsification.&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://repository.upenn.edu/cgi/viewcontent.cgi?article=1528&amp;amp;context=cis_papers&quot;&gt;Design various Graph Sketches.&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;EPFL sublinear algorithm course, 2017&lt;/li&gt;
  &lt;li&gt;EPFL advanced algorithm course, 2016&lt;/li&gt;
  &lt;li&gt;Johnson, William B.; Lindenstrauss, Joram (1984). “Extensions of Lipschitz mappings into a Hilbert space”. In Beals, Richard; Beck, Anatole; Bellow, Alexandra; et al. Conference in modern analysis and probability (New Haven, Conn., 1982). Contemporary Mathematics. 26. Providence, RI: American Mathematical Society. pp. 189–206.&lt;/li&gt;
  &lt;li&gt;Kane, Daniel M.; Nelson, Jelani (2012). “Sparser Johnson-Lindenstrauss Transforms”. Proceedings of the Twenty-Third Annual ACM-SIAM Symposium on Discrete Algorithms,. New York: Association for Computing Machinery (ACM).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The post is used for study purpose only.&lt;/p&gt;

</description>
                <link>/random-projection</link>
                <guid>/Dimensionality-Reduction-via-JL-Lemma-and-Random-Projection</guid>
                <pubDate>Tue, 10 Oct 2017 00:00:00 +0200</pubDate>
        </item>

        <item>
                <title>A comparison of distributed machine learning platform</title>
                <description>&lt;p&gt;A short summary and comparison of different platforms. Based on &lt;a href=&quot;http://muratbuffalo.blogspot.ch/2017/07/a-comparison-of-distributed-machine.html&quot;&gt;this blog&lt;/a&gt; and (Zhang et al., 2017).&lt;/p&gt;

&lt;!-- more --&gt;

&lt;p&gt;We categorize the distributed ML platforms under 3 basic design approaches:
1. basic dataflow
2. parameter-server model
3. advanced dataflow.&lt;/p&gt;

&lt;p&gt;We talk about each approach in brief:
* using Apache Spark as an example of the basic dataflow approach
* PMLS (Petuum) as an example of the parameter-server model
* TensorFlow and MXNet as examples of the advanced dataflow model.&lt;/p&gt;

&lt;h1 id=&quot;spark&quot;&gt;Spark&lt;/h1&gt;
&lt;p&gt;Spark enables in-memory caching of frequently used data and avoids the overhead of writing a lot of intermediate data to disk. For this Spark leverages on Resilient Distributed Datasets (RDD), read-only, partitioned collection of records distributed across a set of machines. RDDs are the collection of objects divided into logical partitions that are stored and processed as in-memory, with shuffle/overflow to disk.&lt;/p&gt;

&lt;p&gt;In Spark, a computation is modeled as a directed acyclic graph (DAG), where each vertex denotes an RDD and each edge denotes an operation on RDD. On a DAG, an edge E from vertex A to vertex B implies that RDD B is a result of performing operation E on RDD A. There are two kinds of operations: transformations and actions. A transformation (e.g., map, filter, join) performs an operation on an RDD and produces a new RDD.&lt;/p&gt;

&lt;p&gt;A typical Spark job performs a couple of transformations on a sequence of RDDs and then applies an action to the latest RDD in the lineage of the whole computation. A Spark application runs multiple jobs in sequence or in parallel.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://4.bp.blogspot.com/-cN_-PWvDGCs/WX6pgpqlTSI/AAAAAAAAGbw/vp4ttIiQ5jAGmjllTEyMrFq200uDWyalQCK4BGAYYCw/s400/sparkArch.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A Spark cluster comprises of a master and multiple workers. A master is responsible for negotiating resource requests made by the Spark driver program corresponding to the submitted Spark application. Worker processes hold Spark executors (each of which is a JVM instance) that are responsible for executing Spark tasks. The driver contains two scheduler components, the DAG scheduler, and the task scheduler. The DAG scheduler is responsible for stage-oriented scheduling, and the task scheduler is responsible for submitting tasks produced by the DAG scheduler to the Spark executors.&lt;/p&gt;

&lt;p&gt;The Spark user models the computation as a DAG which transforms &amp;amp; runs actions on RDDs. The DAG is compiled into stages. Unlike the MapReduce framework that consists of only two computational stages, map and reduce, a Spark job may consist of a DAG of multiple stages. The stages are run in topological order. A stage contains a set of independent tasks which perform computation on partitions of RDDs. These tasks can be executed either in parallel or as pipelined.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://4.bp.blogspot.com/-_KxjkVBsznQ/WX6pcFQ7C5I/AAAAAAAAGbo/GYdLBgVqY78ZEllZ971WoHmBAbnDRayAgCK4BGAYYCw/s400/apache.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Spark defines two types of dependency relation that can capture data dependency among a set of RDDs:
* Narrow dependency. Narrow dependency means each partition of the parent RDD is used by at most one partition of the child RDD.
* Shuffle dependency (wide dependency). Wide dependency means multiple child partitions of RDD may depend on a single parent RDD partition.&lt;/p&gt;

&lt;p&gt;Narrow dependencies are good for efficient execution, whereas wide dependencies introduce bottlenecks since they disrupt pipelining and require communication intensive shuffle operations.&lt;/p&gt;

&lt;h2 id=&quot;fault-tolerance&quot;&gt;Fault tolerance&lt;/h2&gt;
&lt;p&gt;Spark uses the DAG to track the lineage of operations on RDDs. For shuffle dependency, the intermediate records from one stage are materialized on the machines holding parent partitions. This intermediate data is used for simplifying failure recovery. If a task fails, the task will be retried as long as its stage’s parents are still accessible. If some stages that are required are no longer available, the missing partitions will be re-computed in parallel.&lt;/p&gt;

&lt;p&gt;Spark is unable to tolerate a scheduler failure of the driver, but this can be addressed by replicating the metadata of the scheduler. The task scheduler monitors the state of running tasks and retries failed tasks. Sometimes, a slow straggler task may drag the progress of a Spark job.&lt;/p&gt;

&lt;h2 id=&quot;machine-learning-on-spark&quot;&gt;Machine learning on Spark&lt;/h2&gt;
&lt;p&gt;Spark was designed for general data processing, and not specifically for machine learning. However, using the MLlib for Spark, it is possible to do ML on Spark. In the basic setup, Spark stores the model parameters in the driver node, and the workers communicate with the driver to update the parameters after each iteration. For large scale deployments, the model parameters may not fit into the driver and would be maintained as an RDD. This introduces a lot of &lt;strong&gt;overhead&lt;/strong&gt; because a new RDD will need to be created in each iteration to hold the updated model parameters. Updating the model involves shuffling data across machines/disks, this limits the scalability of Spark. This is where the basic dataflow model (the DAG) in Spark falls short. Spark does not support iterations needed in ML well.&lt;/p&gt;

&lt;h1 id=&quot;pmls&quot;&gt;PMLS&lt;/h1&gt;
&lt;p&gt;PMLS was designed specifically for ML with a clean slate. It introduced the parameter-server (PS) abstraction for serving the iteration-intensive ML training process.&lt;/p&gt;

&lt;p&gt;In PMLS, a worker process/thread is responsible for requesting up to date model parameters and carrying out computation over a partition of data, and a parameter-server thread is responsible for storing and updating
model parameters and making response to the request from workers.&lt;/p&gt;

&lt;p&gt;Figure below shows the architecture of PMLS.
&lt;img src=&quot;https://3.bp.blogspot.com/-cFL80lqWCCo/WX6pk2jzcdI/AAAAAAAAGb4/XFYSzGWsD6UPhrewWEll5w61g-vbYAYYwCK4BGAYYCw/s400/pmlsArch.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The parameter server is implemented as distributed tables. All model parameters are stored via these tables. A PMLS application can register more than one table. These tables are maintained by server threads. Each table consists of multiple rows. Each cell in a row is identified by a column ID and typically stores one parameter. The rows of the tables can be stored across multiple servers on different machines.&lt;/li&gt;
  &lt;li&gt;Workers are responsible for performing computation defined by a user on partitioned dataset in each iteration and need to request up to date parameters for its computation. Each worker may contain multiple working threads. There is no communication across workers. Instead, workers only communicate with servers.&lt;/li&gt;
  &lt;li&gt;'’worker’’ and ‘‘server’’ are not necessarily separated physically. In fact server threads co-locate with the worker processes/threads in PMLS.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;error-tolerance-of-ml-algorithm&quot;&gt;Error tolerance of ML algorithm.&lt;/h2&gt;
&lt;p&gt;PMLS exploits the error-tolerant property of many machine learning algorithms to make a trade-off between efficiency and consistency.&lt;/p&gt;

&lt;p&gt;In order to leverage such error-tolerant property, PMLS follows Staleness Synchronous Parallel (SSP) model.  In SSP model, worker threads can proceed without waiting for slow threads.
&amp;gt;  Fast threads may carry out computation using stale model parameters.  Performing computation on stale version of model parameter does cause errors, however these errors are bounded.&lt;/p&gt;

&lt;p&gt;The communication protocol between workers and servers can guarantee that the model parameters that a working thread reads from its local cache is of bounded staleness.&lt;/p&gt;

&lt;h2 id=&quot;fault-tolerance-1&quot;&gt;Fault tolerance&lt;/h2&gt;
&lt;p&gt;Fault tolerance in PMLS is achieved by checkpointing the model parameters in the parameter server periodically. To resume from a failure, the whole system restarts from the last checkpoint.&lt;/p&gt;

&lt;h2 id=&quot;programing-interface&quot;&gt;Programing interface&lt;/h2&gt;
&lt;p&gt;PMLS is written in C++.&lt;/p&gt;

&lt;p&gt;While PMLS has very little overhead, on the negative side, the users of PMLS need to know how to handle computation using relatively low-level APIs.&lt;/p&gt;

&lt;h1 id=&quot;tensorflow&quot;&gt;TensorFlow&lt;/h1&gt;
&lt;p&gt;Tensorflow is the first generation distributed parameter-server system.
In TensorFlow the computation is abstracted and represented by a directed graph. But unlike traditional dataflow systems, TensorFlow allows nodes to represent computations that own or update mutable state.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Variable: a stateful operations, owns mutable buffer, and can be used to store model parameters that need to be updated at each iteration.&lt;/li&gt;
  &lt;li&gt;Node: represents operations, and some operations are control flow operations.&lt;/li&gt;
  &lt;li&gt;Tensors: values that flow along the directed edges in the TensorFlow graph, with arbitrary dimensionality matrices.
    &lt;ul&gt;
      &lt;li&gt;An operation can take in one or more tensors and produce a result tensor.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Edge: special edges called control dependencies can be added into TensorFlow’s dataflow graph with no data flowing along such edges.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In summary, TensorFlow is a dataflow system that offers mutable state and allows cyclic computation graph, and as such enables training a machine learning algorithm with parameter-server model.&lt;/p&gt;

&lt;h2 id=&quot;architecture&quot;&gt;Architecture&lt;/h2&gt;
&lt;p&gt;The Tensorflow runtime consists of three main components: client, master, worker.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;client:  is responsible for holding a session where a user can define computational graph to run. When a client requests the evaluation of a Tensorflow graph via a session object, the request is sent to master service.&lt;/li&gt;
  &lt;li&gt;master: schedules the job over one or more workers and coordinates the execution of the computational graph.&lt;/li&gt;
  &lt;li&gt;worker:  Each worker handles requests from the master and schedules the execution of the kernels (The implementation of an operation on a particular device is called a kernel) in the computational graph. The dataflow executor in a worker dispatches the kernels to local devices and runs the kernels in parallel when possible.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;characteristics&quot;&gt;Characteristics&lt;/h2&gt;
&lt;p&gt;### Node Placement
If multiple devices are involved in computation, a procedure called node placement is executed in a Tensorflow
runtime. Tensorflow uses a cost model to estimate the cost of executing an operation on all available devices (such as CPUs and GPUs) and assigns an operation to a suitable device to execute, subject to implicit or explicit device constraints in the graph.&lt;/p&gt;

&lt;h3 id=&quot;sub-graph-execution&quot;&gt;Sub-graph execution&lt;/h3&gt;
&lt;p&gt;TensorFlow supports sub-graph execution. A single round of executing a graph/sub-graph is called a step.&lt;/p&gt;

&lt;p&gt;A training application contains two type of jobs: parameter server (ps) job and worker job. Like data parallelism in PMLS, TensorFlow’s data parallelism training involves multiple tasks in a worker job training the same model on different minibatches of data, updating shared parameters hosted in a one or more tasks in a ps job.&lt;/p&gt;

&lt;h3 id=&quot;a-typical-replicated-training-structure-between-graph-replication&quot;&gt;A typical replicated training structure: between-graph replication&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://1.bp.blogspot.com/-LToYY4Kj2YE/WX6pod_r5pI/AAAAAAAAGcA/Ls-ZWfTebYk_sc3l2pCHRAWv9e6U_eT_gCK4BGAYYCw/s400/tf.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There is a separate client for each worker task, typically in the same process as the worker task. Each client builds a similar graph containing the parameters (pinned to ps) and a single copy of the compute-intensive part of the computational graph that is pinned to the local task in the worker job.&lt;/p&gt;

&lt;p&gt;For example, a compute-intensive part is to compute gradient during each iteration of stochastic gradient descent algorithm.&lt;/p&gt;

&lt;p&gt;Users can also specify the consistency model in the betweengraph replicated training as either synchronous training or asynchronous training:
*  In asynchronous mode, each replica of the graph has an independent training loop that executes without coordination.
* In synchronous mode, all of the replicas read the same values for the current parameters, compute gradients in parallel, and then apply them to a stateful accumulators which act as barriers for updating variables.&lt;/p&gt;

&lt;h2 id=&quot;fault-tolerance-2&quot;&gt;Fault tolerance&lt;/h2&gt;
&lt;p&gt;TensorFlow provides user-controllable checkpointing for fault tolerance via primitive operations: &lt;em&gt;save&lt;/em&gt; writes tensors to checkpoint file, and &lt;em&gt;restore&lt;/em&gt; reads tensors from a checkpointing file.
TensorFlow allows customized fault tolerance mechanism through its primitive operations, which provides users the ability to make a balance between reliability and checkpointing overhead.&lt;/p&gt;

&lt;h1 id=&quot;mxnet&quot;&gt;MXNET&lt;/h1&gt;
&lt;p&gt;Similar to TensorFlow, MXNet is a dataflow system that allows cyclic computation graphs with mutable states, and supports training with parameter server model. Similar to TensorFlow, MXNet provides good support for data-parallelism on multiple CPU/GPU, and also allows model-parallelism to be implemented.
MXNet allows both synchronous and asynchronous training.&lt;/p&gt;

&lt;h2 id=&quot;characteristics-1&quot;&gt;Characteristics&lt;/h2&gt;
&lt;p&gt;Figure below illustrates main components of MXNet. The runtime dependency engine analyzes the dependencies in computation processes and parallelizes the computations that are not dependent. On top of runtime dependency engine, MXNet has a middle layer for graph and memory optimization.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/dmlc/dmlc.github.io/master/img/mxnet/system/overview.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;fault-tolerance-3&quot;&gt;Fault tolerance&lt;/h2&gt;
&lt;p&gt;MXNet supports basic fault tolerance through checkpointing, and provides save and load model operations. The save operaton writes the model parameters to the checkpoint file and the load operation reads model parameters from the checkpoint file.&lt;/p&gt;

&lt;h1 id=&quot;reference&quot;&gt;Reference&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;Zhang, Kuo and Alqahtani, Salem and Demirbas, Murat, ‘A Comparison of Distributed Machine Learning Platforms’, ICCCN, 2017.
The post is used for study purpose only.&lt;/li&gt;
&lt;/ul&gt;

</description>
                <link>/comparison-distributed-ml-platform</link>
                <guid>/a-comparison-of-distributed-machine-learning-platform</guid>
                <pubDate>Thu, 07 Sep 2017 00:00:00 +0200</pubDate>
        </item>

        <item>
                <title>Bias-variance decomposition in a nutshell</title>
                <description>&lt;p&gt;This post means to give you a nutshell description of bias-variance decomposition.&lt;/p&gt;

&lt;h2 id=&quot;basic-setting&quot;&gt;basic setting&lt;/h2&gt;
&lt;p&gt;We will show four key results using Bias-variance decomposition.&lt;/p&gt;

&lt;p&gt;Let us assume $f_{true}(x_n)$ is the true model, and the observations are given by:&lt;/p&gt;

&lt;p&gt;\[y_n = f_{true}(x_n) + \epsilon_n \qquad (1)\]&lt;/p&gt;

&lt;p&gt;where $\epsilon_n$ are i.i.d. with zero mean and variance $\sigma^2$. Note that $f_{true}$ can be nonlinear and $\epsilon_n$ does not have to be Gaussian.&lt;/p&gt;

&lt;p&gt;We denote the least-square estimation by&lt;/p&gt;

&lt;p&gt;\[f_{lse}(x_{\ast}) = \tilde{x}_{\ast}^T w_{lse} \]&lt;/p&gt;

&lt;p&gt;Where the tilde symbol means there is a constant 1 feature added to the raw data. For this derivation, we will assume that $x_{\ast}$ is fixed, although it is straightforward to generalize this.&lt;/p&gt;

&lt;h2 id=&quot;expected-test-error&quot;&gt;Expected Test Error&lt;/h2&gt;
&lt;p&gt;Bias-variance comes directly out of the test error:&lt;/p&gt;

&lt;p&gt;\[ \overline{teErr} = \mathbb{E}[(observation - prediction)^2] \qquad (2.1) \]&lt;/p&gt;

&lt;p&gt;\[   =\mathbb{E}_{D_{tr},D_{te}} [(y_\ast − f_{lse})^2] \qquad (2.2)\]&lt;/p&gt;

&lt;p&gt;\[   = \mathbb{E}_{y_\ast,w_{lse}} [(y_\ast −f_{lse} )^2] \qquad (2.3)\]&lt;/p&gt;

&lt;p&gt;\[ = \mathbb{E}_{y_\ast, w_{lse}} [(y_\ast −f_{true} + f_{true} −f_{lse})^2]  \qquad (2.4) \]&lt;/p&gt;

&lt;p&gt;\[ = \mathbb{E}_{y_\ast}[(y_{\ast}−f_{true})^2] + \mathbb{E}_{w_{lse}} [(f_{lse} − f_{true})^2] \qquad (2.5)\]&lt;/p&gt;

&lt;p&gt;\[ = \sigma^2 + \mathbb{E} w_{lse} [(f_{lse} − \mathbb{E} w_{lse} [f_{lse}] −f_{true} + \mathbb{E}w_{lse} [f_{lse}])^2]  \qquad (2.6)\]&lt;/p&gt;

&lt;p&gt;\[ = \sigma^2 + \mathbb{E} w_{lse} [(f_{lse} − \mathbb{E} w_{lse} [f_{lse}])^2] +  [f_{true} + \mathbb{E}w_{lse} (f_{lse})]^2  \qquad (2.7)\]&lt;/p&gt;

&lt;p&gt;(I am sorry, I did not find the equation alignment in MathJax.) Where equation (2.2) is the expectation over training data and testing data; and the second term in equation (2.7) is called &lt;strong&gt;predict variance&lt;/strong&gt;, and the third term of it is called the square of &lt;strong&gt;predict bias&lt;/strong&gt;. Thus comes the name bias-variance decomposition.&lt;/p&gt;

&lt;h2 id=&quot;where-does-the-bias-come-from-model-bias-and-estimation-bias&quot;&gt;Where does the bias come from? model bias and estimation bias&lt;/h2&gt;
&lt;p&gt;As illustrated in the following figure, bias comes from model bias and estimation bias. Model bias comes from the model itself; and estimation bias comes from dataset (mainly). And bear in mind that ridge regression increases estimation bias while reducing variance(you may need to find other papers to get this idea)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/imgblog/bias-variance.png&quot; alt=&quot;Where does bias come from?&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Kevin, Murphy. “Machine Learning: a probabilistic perspective.” (2012).&lt;/li&gt;
  &lt;li&gt;Bishop, Christopher M. “Pattern recognition.” Machine Learning 128 (2006).&lt;/li&gt;
  &lt;li&gt;Emtiyaz Khan’s lecture notes on PCML, 2015&lt;/li&gt;
&lt;/ul&gt;

</description>
                <link>/bias-variance</link>
                <guid>/biasvariance</guid>
                <pubDate>Thu, 08 Dec 2016 00:00:00 +0100</pubDate>
        </item>

        <item>
                <title>On Saddle Points: a painless tutorial</title>
                <description>&lt;p&gt;Are we really stuck in the local minima rather than anything else?&lt;/p&gt;

&lt;h2 id=&quot;different-types-of-critical-points&quot;&gt;Different types of critical points&lt;/h2&gt;
&lt;hr /&gt;

&lt;div class=&quot;fig figcenter fighighlight&quot;&gt;
  &lt;img src=&quot;/assets/blog/updatemethods/minmaxsaddle.png&quot; width=&quot;100%&quot; /&gt;
  &lt;div class=&quot;figcaption&quot;&gt;
    Various Types of Critical Points. Source: Rong Ge's blog.
  &lt;/div&gt;
&lt;/div&gt;
&lt;hr /&gt;

&lt;p&gt;To minimize the function \(f:\mathbb{R}^n\to \mathbb{R}\), the most popular approach is to follow the opposite direction of the gradient \(\nabla f(x)\) (for simplicity, all functions we talk about are infinitely differentiable), that is,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y = x - \eta \nabla f(x),&lt;/script&gt;

&lt;p&gt;Here \(\eta\) is a small step size. This is the &lt;em&gt;gradient descent&lt;/em&gt; algorithm.&lt;/p&gt;

&lt;p&gt;Whenever the gradient \(\nabla f(x)\) is nonzero, as long as we choose a small enough \(\eta\), the algorithm is guaranteed to make &lt;em&gt;local&lt;/em&gt; progress. When the gradient \(\nabla f(x)\) is equal to \(\vec{0}\), the point is called a &lt;strong&gt;critical point&lt;/strong&gt;, and gradient descent algorithm will get stuck. For (strongly) convex functions, there is a unique &lt;em&gt;critical point&lt;/em&gt; that is also the &lt;em&gt;global minimum&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;However, this is not always this case. All critical points of \( f(x) \) can be further characterized by the curvature of the function in its vicinity, especially described by it’s eigenvalues of the Hessian matrix. Here I describe three possibilities as the figure above shown:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;If all eigenvalues are non-zero and positive, then the critical point is a local minimum.&lt;/li&gt;
  &lt;li&gt;If all eigenvalues are non-zero and negative, then the critical point is a local maximum.&lt;/li&gt;
  &lt;li&gt;If the eigenvalues are non-zero, and both positive and negative eigenvalues exist, then the critical point is a saddle point.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The proof of the above three possibilities can be shown from the reparametrization of the space of Hessian matrix. The Taylor expansion is given by(first order derivative vanishes):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x+\Delta x) = f(x) + \frac{1}{2} (\Delta x)^T \mathbf{H} \Delta x \,\,\,\, -----  \,\,\,(1)&lt;/script&gt;

&lt;p&gt;And assume \(\mathbf{e_1}, \mathbf{e_2}, …, \mathbf{e_n}\) are the eigenvectors and \(\lambda_1, \lambda_2, …, \lambda_n\) are the eigenvalues correspondingly. We can make the reparametrization of the space by:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Delta v = \frac{1}{2} \begin{bmatrix} \mathbf{e_1}^T\\ ... \\ \mathbf{e_n}^T \end{bmatrix} \Delta x&lt;/script&gt;

&lt;p&gt;Then combined with Taylor expansion, we can get the following equation:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x+ \Delta x) = f(x)+\frac{1}{2} \sum_{i=1}^n \lambda_i(\mathbf{e_i}^T \Delta x)^2 = f(x) + \sum_{i=1}^n \lambda_i \Delta \mathbf{v_i}^2&lt;/script&gt;

&lt;p&gt;For the proof of the above equation, you may need to look at &lt;a href=&quot;https://inst.eecs.berkeley.edu/~ee127a/book/login/l_sym_sed.html&quot;&gt;Spectrum Theorem&lt;/a&gt;, which is related to the eigenvalues and eigenvectors of symmetric matrices.&lt;/p&gt;

&lt;p&gt;From this equation, all the three scenarios for critical points are self-explained.&lt;/p&gt;

&lt;h2 id=&quot;first-order-method-to-escape-from-saddle-point&quot;&gt;First order method to escape from saddle point&lt;/h2&gt;
&lt;p&gt;A &lt;a href=&quot;http://www.offconvex.org/2016/03/22/saddlepoints/&quot;&gt;post&lt;/a&gt; by Rong Ge introduced a first order method to escape from saddle point. He claimed that saddle points are very &lt;em&gt;unstable&lt;/em&gt;: if we put a ball on a saddle point, then slightly perturb it, the ball is likely to fall to a local minimum, especially when the second order term \(\frac{1}{2} (\Delta x)^T \mathbf{H} \Delta x\) is significantly smaller than 0(there is a steep direction where the function value decrease, and assume we are looking for local minimum), which is called a &lt;em&gt;Strict Saddle Function&lt;/em&gt; in Rong Ge’s post. In this case we can use &lt;em&gt;noisy gradient descent&lt;/em&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;\(y = x - \eta \nabla f(x) + \epsilon.\)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;where \(\epsilon\)  is a noise vector that has mean \(\mathbf{0}\). Actually, it is the basic idea of &lt;em&gt;stochastic gradient descent&lt;/em&gt;, which uses the gradient of a mini batch rather than the true gradient. However, the drawback of the stochastic gradient descent is not the direction, but the size of the step along each eigenvector. The step, along any direction \(\mathbf{e_i}\), is given by \(-\lambda_i \Delta \mathbf{v_i}\), when the steps taken in the direction with small absolute value of eigenvalues, the step is small. To be more concrete, an example that the curvature of the error surface may not be the same in all directions. If there is a long and narrow valley in the error surface, the component of the gradient in the direction that points along base of the valley is very small while the component perpendicular to the valley walls is quite large even though we have to move a long distance along the base and a small distance perpendicular to the walls. This phenomenon can be seen as the following figure:&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;fig figcenter fighighlight&quot;&gt;
  &lt;img src=&quot;/assets/blog/updatemethods/without_momentum.png&quot; width=&quot;70%&quot; /&gt;
  &lt;div class=&quot;figcaption&quot;&gt;
    SGD optimization routes
  &lt;/div&gt;
&lt;/div&gt;
&lt;hr /&gt;

&lt;p&gt;We normally move by making a step that is some constant times the negative gradient rather than a step of constant length in the direction of the negative gradient. This means that in steep regions (where we have to be careful not to make our steps too large), we move quickly, and in shallow regions (where we need to move in big steps), we move slowly.&lt;/p&gt;

&lt;h2 id=&quot;newton-methods&quot;&gt;Newton methods&lt;/h2&gt;
&lt;p&gt;To look at the detail of newton methods, you can follow the proof shown in (Sam Roweis’s) in the reference list. The newton method solves the slowness problem by rescaling the gradients in each direction with the inverse of the corresponding eigenvalue, yielding the step \(-\Delta \mathbf{v_i}\)(because \(\frac{1}{\lambda_i}\mathbf{e_i} = \mathbf{H}^{-1}\mathbf{e_i}  \) ). However, this approach can result in moving in the wrong direction when the eigenvalue is negative. The newton step moves along the eigenvector in a direction &lt;strong&gt;opposite&lt;/strong&gt; to the gradient descent step, thus increase the error.&lt;/p&gt;

&lt;p&gt;From the idea of Levenberg gradient descent method, we can use damping, in which case we remove negative curvature by adding a constant \(\alpha\) to its diagonal. Informally, \(x^{k+1} = x^{k} - (\mathbf{H}+\alpha \mathbf{I})^{-1} \mathbf{g_k}\). We can view \(\alpha\) as the tradeoff between newton methods and gradient descent. When \(\alpha\) is small, it is closer to newton method, when \(\alpha\) is large, it is closer to gradient descent. In this case, we get the step \(-\frac{\lambda_i}{\lambda_i + \alpha}\Delta \mathbf{v_i}\). Therefore, obviously, the drawback of damping newton method is that it potentially has small step size in many eigen-directions incurred by large damping factor \(\alpha\).&lt;/p&gt;

&lt;h2 id=&quot;saddle-free-newton-method&quot;&gt;Saddle free newton method&lt;/h2&gt;
&lt;p&gt;(Dauphin et al., 2014) introduced a method called saddle free newton method, which is a modified version of trust region approach. It minimizes first-order Taylor expansion constraint by the distance between first-order Taylor expansion and second-order Taylor expansion. By this constraint, unlike gradient descent, it can move further in the directions of low curvature; and move less in the directions of high curvature. I recommend you to read this paper throughly.&lt;/p&gt;

&lt;h2 id=&quot;future-post&quot;&gt;Future post&lt;/h2&gt;
&lt;p&gt;I have talked about degenerate critical point in this post， where there are only positive and zero eigenvalues in the Hessian matrix.&lt;/p&gt;

&lt;h2 id=&quot;marks&quot;&gt;Marks&lt;/h2&gt;

&lt;p&gt;The slides for the talk of this blog can be found at &lt;a href=&quot;http://www.junlulocky.com/assets/talks/2016onsaddlepoints.pdf&quot;&gt;Link&lt;/a&gt;. Contact me if the link is not working.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://inst.eecs.berkeley.edu/~ee127a/book/login/l_sym_sed.html&quot;&gt;Berkeley Optimization Models: Spectral Theorem&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Dauphin, Yann N., et al. &lt;em&gt;Identifying and attacking the saddle point problem in high-dimensional non-convex optimization.&lt;/em&gt; Advances in neural information processing systems. 2014.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.cs.nyu.edu/~roweis/notes/lm.pdf&quot;&gt;Sam Roweis’s note on Levenberg-Marquardt Optimization&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Rong Ge, &lt;em&gt;Escaping from Saddle Points&lt;/em&gt;, Off the convex path blog, 2016&lt;/li&gt;
  &lt;li&gt;Benjamin Recht, &lt;em&gt;Saddles Again&lt;/em&gt;, Off the convex path blog, 2016&lt;/li&gt;
&lt;/ul&gt;

</description>
                <link>/saddle-points</link>
                <guid>/SaddlePoints</guid>
                <pubDate>Wed, 07 Sep 2016 00:00:00 +0200</pubDate>
        </item>

        <item>
                <title>Normalizations in Neural Networks</title>
                <description>&lt;p&gt;This post will introduce some normalization related tricks in neural networks.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;normalization-and-equalization&quot;&gt;Normalization and Equalization&lt;/h2&gt;
&lt;p&gt;In image process area, the term “normalization” has many other names such as contrast stretching, histogram stretching or dynamic range expansion etc.
If you have an 8-bit grayscale image, the minimum and maximum pixel values are 50 and 180, we can normalize this image to a larger dynamic range say 0 to 255. After normalize, the previous 50 becomes 0, and 180 becomes 255, the values in the middle will be scaled according to the following formula:&lt;/p&gt;

&lt;p&gt;(I_n: new_intensity) = ((I_o: old_intensity)- (I_o_min: old_minimum_intensity)) x ((I_n_max: new_maximum_intensity) - (I_n_min: new_minimum_intensity)) / ((I_o_max: old_maximum_intensity) - (I_o_min: old_minimum_intensity)) + (I_n_min: new_minimum_intensity)&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;http://yeephycho.github.io/blog_img/normalization.jpg&quot; alt=&quot;Normalization&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It’s a typical linear transform. Still the previous image, the pixel value 70 will become (70-50)x(255-0)/(180-50) - 0 = 39, the pixel value 130 will become (130-50)x(255-0)/(180-50) - 0 = 156.
The image above shows the effect of an image before and after normalization, the third image is effect of another transform called &lt;a href=&quot;https://en.wikipedia.org/wiki/Histogram_equalization&quot;&gt;histogram equalization&lt;/a&gt;, for your information, histogram equalization is different from normalization, normalization will not change your image’s histogram but equalization will. Histogram equalization doesn’t care about intensity value of the pixel, however, the ranking of the current intensity in the whole image matters a lot.
The maximum intensity of the original image is 238 and the minimum is 70, implemented through OpenCV. (OpenCV’s normalization function isn’t the normalization we are talking about, if you want to repeat the effect, you have to do it yourself.)
For normalization, the new intensity derives from the new and old maximum, minimum intensity; for equalization, the new intensity derives from the intensity value’s ranking in the whole image (for example, a image has 64 pixels, the intensity of a certain pixel is 90, and there are 22 pixels has a low intensity and 41 pixels has a higher intensity, the new intensity after equalization of that point is (22/64) x (255-0) = 87).&lt;/p&gt;

&lt;h2 id=&quot;simplified-whitening&quot;&gt;Simplified Whitening&lt;/h2&gt;
&lt;p&gt;Real whitening process is a series of linear transform to make the data have zero means and unit variances, and decorrelated. And there’s quite a lot of math, I don’t really want to talk too much about the math. (Editing formulas are quite annoying, you know.)
As issued above, the propose of whitening or ICA (Independent Compoment Analysis) or sphering is to get ride of the correlations among the raw data. Let’s say, for an image, there’s a high chance that the adjacent pixel’s intensity is similar, this kind of similarity over the spatial domain is the so called correlation, and ICA is a way to reduce this similarity.
Usually, in neural networks we use simplified whitening instead of original ICA, because the computation burden for ICA is just too heavy for big data (say millions of images).
&lt;img src=&quot;http://yeephycho.github.io/blog_img/simplified_whitening.jpg&quot; alt=&quot;simplified whitening&quot; /&gt;
Too tired to explain this formula, maybe later, forgive me…&lt;/p&gt;

&lt;p&gt;Let’s presume you have 100 grayscale images to process, each image has a width of 64 and height of 64, conventions are described as follows:
-	First, calculate the mean and standard deviation (square root of variance) for pixels that has the same x and y coordinate.
-	Then, for each pixel, subtract the mean and divide the standard deviation.&lt;/p&gt;

&lt;p&gt;For example, among the 100 image, get the intensity for pixels at the position (0, 0), you  will have 100 intensity values, calculate the mean and standard deviation for these 100 values. And then, for each pixel of these 100 values, subtracts the mean and divides the variance. And then repeat the same process for other pixels at other positions, in this example, you should iterate it for 64x64 times in total.
After the above process, each dimension of the data set along the batch axis has a zero mean and unit variance. The similarity has already been reduced (for my understanding, the first order similarity has already gone, but the higher order similarity is still there, that’s where the real ICA takes place, to wipe out the higher order similarity).
By doing whitening, the network will converge faster than without whitening.
## Local Constrast Normalization (LCN)
Related papers are listed below:
&lt;a href=&quot;http://journals.plos.org/ploscompbiol/article?id=10.1371%2Fjournal.pcbi.0040027&quot;&gt;Why is Real-World Visual Object Recognition Hard?&lt;/a&gt;, published in Jan. 2008. At this time, the name is “Local input divisive normalization”.
&lt;a href=&quot;http://www.cns.nyu.edu/pub/lcv/lyu08b.pdf&quot;&gt;Nonlinear Image Representation Using Divisive Normalization&lt;/a&gt;, published in Jun. 2008. The name is “Divisive Normalization”.
&lt;a href=&quot;http://yann.lecun.com/exdb/publis/pdf/jarrett-iccv-09.pdf&quot;&gt;What is the Best Multi-Stage Architecture for Object Recognition?&lt;/a&gt;, released in 2009. The name is “Local Constrast Normalization”.
Whitening is a way to normalize the data in different dimensions to reduce the correlations among the data, however, local contrast normalization, whose idea is inspired by computational neuroscience, aims at to make the features in feature maps more significant.&lt;/p&gt;

&lt;p&gt;This (Local Constrast Normalization) module performs local subtraction and division normalizations, enforcing a sort of local competition between adjacent features in a feature map, and between features at the same spatial location in different feature maps.&lt;/p&gt;

&lt;p&gt;Local contrast normalization is implemented as follows:
+	First, for each pixel in a feature map, find its adjacent pixels. Let’s say the radius is 1, so there are 8 pixels around the target pixel (do the zero padding if the target is at the edge of the feature map).
+	Then, compute the mean of these 9 pixels (8 neighbor pixels and the target pixel itself), subtract the mean for each one of the 9 pixels.
+	Next, compute the standard deviation of these 9 pixels. And judge whether the standard deviation is larger then 1. If larger than 1, divide the target pixel’s value (after mean subtraction) by the standard deviation. If not larger, keep the target’s value as they what they are (after mean subtraction).
+	At last, save the target pixel value to the same spatial position of a blank feature map as the input of the following CNN stages.&lt;/p&gt;

&lt;p&gt;I typed the following python code to illustrate the math of the LCN:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;255&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# generate a random 3x3 matrix, the pixel value is ranging from 0 to 255.&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;201&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;239&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;77&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;139&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;157&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;23&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;235&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;207&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;173&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;161.2222222222223&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;39.77777778&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;77.77777778&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;84.22222222&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;22.22222222&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;4.22222222&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;138.22222222&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;73.77777778&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;45.77777778&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;11.77777778&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std_var&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;var&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;）&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std_var&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;68.328906&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std_var&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LCN_value&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std_var&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LCN_value&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0617926207&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Please be noted that the real process in the neural network is not looks like this, because the data is usually whitened before feed to the network, the image usually isn’t randomly generated and the negative value is usually set to zero in ReLU.
Here, we presume that each adjacent pixel has the same importance to the contrast normalization so we calculate the mean of the 9 pixels, actually, the weights for each pixel can be various.
We, presume the adjacent pixel radius is 1 and the image has only one channel, but the radius can be larger or smaller, you can pick up 4 adjacent pixels (up, down, left, right) or 24 pixels (radius is 2) or arbitrary pixels at arbitrary positions (the result may looks odd).
In the third paper, they introduced the divisive normalization into neural networks, and there is variation, that is the contrast normalization among adjacent feature maps at the same spatial position (say a pixel select two adjacent feature maps, the neighbor pixel number is 3x3x3 - 1). In conv. neural network, the output of a layer may have may feature maps, and the LCN can enhance feature presentations in some feature maps at the mean time restrain the presentations in other feature maps.
## Local Response Normalization (LRN)
This concept was raised in AlexNet, click &lt;a href=&quot;http://yeephycho.github.io/2016/07/21/A-reminder-of-algorithms-in-Convolutional-Neural-Networks-and-their-influences-I/&quot;&gt;here&lt;/a&gt; to learn more.
Local response normalization algorithm was inspired by the real neurons, as the author said, “bears some resemblance to the local contrast normalization”. The common point is that they both want to introduce competitions to the neuron outputs, the difference is LRN do not subtract mean and the competition happens among the outputs of adjacent kernels at the same layer.
The formula for LRN is as follows:
&lt;img src=&quot;http://yeephycho.github.io/blog_img/local_response_normalization.jpg&quot; alt=&quot;Local Response Normalization&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;a(i, x, y)&lt;/em&gt;&lt;/strong&gt; represents the &lt;em&gt;i&lt;/em&gt; th conv. kernel’s output (after ReLU) at the position of (x, y) in the feature map.
&lt;strong&gt;&lt;em&gt;b(i, x, y)&lt;/em&gt;&lt;/strong&gt; represents the output of local response normalization, and of course it’s also the input for the next layer.
&lt;strong&gt;&lt;em&gt;N&lt;/em&gt;&lt;/strong&gt; is the number of the conv. kernel number.
&lt;strong&gt;&lt;em&gt;n&lt;/em&gt;&lt;/strong&gt; is the adjacent conv. kernel number, this number is up to you. In the article they choose n = 5.
&lt;strong&gt;&lt;em&gt;k, α， β&lt;/em&gt;&lt;/strong&gt; are hyper-parameters, in the article, they choose &lt;strong&gt;&lt;em&gt;k = 2, α = 10e-4, β = 0.75&lt;/em&gt;&lt;/strong&gt;.
&lt;img src=&quot;http://yeephycho.github.io/blog_img/local_response_normalization_process.jpg&quot; alt=&quot;Local Response Normalization illustration&quot; /&gt;
Flowchart of Local Response Normalization&lt;/p&gt;

&lt;p&gt;I drew the above figure to illustrate the process of LRN in neural network. Just a few tips here:
- This graph presumes that the &lt;em&gt;i&lt;/em&gt; th kernel is not at the edge of the kernel space. If i equals zero or one or last or one to the last, one or two additional zero padding conv. kernels are required.
- In the article, n is 5, we presume n/2 is integer division, 5/2 = 2.
- Summation of the squares of output of ReLU stands for: for each output of ReLU, compute its square, then, add the 5 squared value together. This process is the summation term of the formula.
- I presume the necessary padding is used by the input feature map so that the output feature maps have the same size of the input feature map, if you really care. But this padding may not be quite necessary.&lt;/p&gt;

&lt;p&gt;After knowing what LRN is, another question is: what the output of LRN looks like?
Because the LRN happens after ReLU, so the inputs should all be no less than 0. The following graph tries to give you an intuitive understanding on the output of LRN, however, you still need to use your imagination.
&lt;img src=&quot;http://yeephycho.github.io/blog_img/LRN.png&quot; alt=&quot;Local Response Normalization output&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Be noted that the x axis represents the summation of the squared output of ReLU, ranging from 0 to 1000, and the y axis represents b(i, x, y) divides a(i, x, y). The hyper-parameters are set default to the article.
So, the real b(i, x, y)’s value should be the the y axis’s value multiplied with the a(i, x, y), use your imagination here, two different inputs a(i, x, y) pass through this function. Since the slope at the beginning is very steep, little difference among the inputs will be significantly enlarged, this is where the competition happens.
The figure was generated by the following python code:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;lrn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;...&lt;/span&gt;   &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;10e-4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.75&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;...&lt;/span&gt;   &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lrn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'sum(x^2)'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'1 / (k + a * sum(x^2))'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;batch-normalization&quot;&gt;Batch Normalization&lt;/h2&gt;
&lt;p&gt;I summarized related paper in &lt;a href=&quot;http://yeephycho.github.io/2016/08/02/A-reminder-of-algorithms-in-Convolutional-Neural-Networks-and-their-influences-II/&quot;&gt;another blog&lt;/a&gt;.
Batch normalization, at first glance, is quite difficult to understand. It truly introduced something new to CNNs, that is a kind of learnable whitening process to the inputs of the non-linear activations(ReLUs or Sigmoids).
You can view the BN operation (represented as op. at the rest of this post) as a simplified whitening on the data in the intermittent layer of the neural network. In the original paper, I think the BN op. happens after the conv. op. but before the ReLU or Sigmoid op.
But, BN is not that easy, because, first, the hyper-parameters “means” and “variances” are learned through back-propagation, and the training is mini-batch training either online training nor batch training. I’m going to explain these ideas below.
First, let’s review the simplified whitening formula:
&lt;img src=&quot;http://yeephycho.github.io/blog_img/simplified_whitening.jpg&quot; alt=&quot;simplified whitening&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Then, follow the similar idea, batch normalization defined two trainable parameters one comes from the mean, the other comes from the variance (or variance’s square root-standard deviation), view the algorithms and formula sets in page 3 and 4 in &lt;a href=&quot;https://arxiv.org/pdf/1502.03167.pdf&quot;&gt;original paper&lt;/a&gt;.
&lt;img src=&quot;http://yeephycho.github.io/blog_img/bn_train.jpg&quot; alt=&quot;Batch normalization algorithms&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Online training means that when you train your network, each time you feed only one instance to your network, calculate the loss at the last layer and based on the loss of this single instance, using back-propagation to adjust your network’s parameters. Batch training means when you train your network, you feed all your data to the network, and calculate the loss of the whole dataset, based on the total loss do be BP learning. Mini-batch training means you feed a small part of your training data to the network, then, calculate the total loss of the small part of the data at the last layer, then based on the loss of this small part of data do the BP learning.
Online training usually suffers from the noise the adjustment is usually quite noisy but if your training is implemented on a single thread CPU, online training is believed to be the fastest scheme and you can use larger learning rate.
Batch training has a better estimation on the gradient, so the training can be less noisy, but batch training should be carefully initialized and the learning rate should be small, so the training speed is believed to be slow.
Mini-batch training is a compromise between online training and the batch training. It uses a batch of data to estimate the gradient, so the learning is less noisy. Batch training and mini-batch training all can take advantage of the parallel computing such as multi-thread computing or GPU computing. So, the speed is much faster than single thread training.
Batch normalization of course uses batch training. In ImageNet classification, they choose the batch size of 32, that is every time they feed 32 images to the network to calculate the loss and estimate the error. Each image is 224*224 pixels, so each batch has 50176 dimensions.
Let’s take out a intermittent conv. layer to illustrate the BN op.
&lt;img src=&quot;http://yeephycho.github.io/blog_img/bn_process.jpg&quot; alt=&quot;Batch normalization process&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The trick part is γ and β are initialized by the batch standard deviations and means but trained subject to the loss of the network through back-propagation.
Why? Why we need to train γ and β instead of using the standard deviation and mean of the batch directly, you may think it’s possibly a better way to reduce the correlations or shifts. The reason is that it can be proved or observed that by naive subtracting mean and dividing the variance, there’s no help to the network, take mean as an example, the bias unit in the network will make up the loss of the mean.
In my opinion, batch normalization is trying to find a balance between the simplified whitening and raw. They issued in the paper, the initial transform is an identity transform to the data. After γ and β were trained, I believe that the transform is not identity anymore. And they also say BN is a way to solve the internal covariate shift, to solve the problem of shifting in the distribution of the inputs in different layers, according to their description, BN is a significant improvement to the network architecture, I believe it’s true but I don’t think they can really get ride of the distribution shift, as the title of the paper said, it can improve the network by “Reducing Internal Covariate Shift”.
The last thing to address, when using the network trained by BN to do the inference, a further process to γ and β are needed, you can find how to implement the process according to the 8, 9, 10, 11 lines in Alg. 2. The idea is that the trained γ and β in the model need to be further normalized by the &lt;a href=&quot;https://en.wikipedia.org/wiki/Maximum_likelihood_estimation&quot;&gt;maximum likelihood estimation&lt;/a&gt; of global variance and mean.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
## License&lt;/p&gt;

&lt;p&gt;The content of this blog itself is licensed under the &lt;a href=&quot;https://creativecommons.org/licenses/by-sa/4.0/&quot;&gt;Creative Commons Attribution 4.0 International License&lt;/a&gt;.
&lt;img src=&quot;http://yeephycho.github.io/blog_img/license.jpg&quot; alt=&quot;CC-BY-SA LICENCES&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The containing source code (if applicable) and the source code used to format and display that content is licensed under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot;&gt;Apache License 2.0&lt;/a&gt;.
Copyright [2016] [yeephycho]
Licensed under the Apache License, Version 2.0 (the “License”);
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
&lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot;&gt;Apache License 2.0&lt;/a&gt;
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an “AS IS” BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
express or implied. See the License for the specific language
governing permissions and limitations under the License.
&lt;img src=&quot;http://yeephycho.github.io/blog_img/APACHE.jpg&quot; alt=&quot;APACHE LICENCES&quot; /&gt;&lt;/p&gt;
</description>
                <link>/normalizations-in-neural-networks</link>
                <guid>/Normalizations-in-neural-networks</guid>
                <pubDate>Wed, 03 Aug 2016 00:00:00 +0200</pubDate>
        </item>

        <item>
                <title>The math behind Gradient Descent</title>
                <description>&lt;p&gt;This post means to help starters to understand the math behind Gradient Descent (GD).&lt;/p&gt;

&lt;h2 id=&quot;intuitive-understanding&quot;&gt;Intuitive understanding&lt;/h2&gt;
&lt;p&gt;An intuitive way to think of Gradient Descent is to imagine the path of a river originating from top of a mountain. The goal of gradient descent is exactly what the river strives to achieve - namely, reach the bottom most point (at the foothill) climbing down from the mountain. That’s what you taught by your machine learning teacher right? But do you only understand this and use gradient descent naively every time? This post will help you understand the math behind gradient descent.&lt;/p&gt;

&lt;h2 id=&quot;math-behind-gradient-descent&quot;&gt;Math behind Gradient Descent&lt;/h2&gt;
&lt;p&gt;Here I define the objective function to be \(L(x,w,b)\) and the input variable of \(L\) is \(x\) with \(d\)-dimension, weight variable \(w\) and bias variable\(b\), our goal is to use algorithm to get the minimum of \(L(x,w,b)\).&lt;/p&gt;

&lt;p&gt;To make this question more precise, let’s think about what happens when we move the ball a small amount \(\Delta x_1\) in the \(x_1\) direction, a small amount \(\Delta x_2\) in the \(x_2\) direction, …, and a small amount \(\Delta x_d\) in the \(x_d\) direction. Calculus tells us that \(L(x,w,b)\) changes as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Delta L \approx \frac{\partial L}{\partial x_1}\Delta x_1 + ... + \frac{\partial L}{\partial x_d}\Delta x_d&lt;/script&gt;

&lt;p&gt;In this sense, we need to find a way of choosing \(\Delta x_1\), …, \(\Delta x_d\) so as to make \(\Delta L\) negative; i.e., we’ll make the objective function decrease so that to minimize.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Define \(\Delta x=(\Delta x_1, …, \Delta x_d)^T\) to be the vector of changes in \(x\).&lt;/li&gt;
  &lt;li&gt;Define \(\nabla L=(\frac{\partial L}{\partial x_1}, …, \frac{\partial L}{\partial x_d})^T\) to be the gradient vector of \(L\).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So we can find: &lt;script type=&quot;math/tex&quot;&gt;\Delta L \approx \frac{\partial L}{\partial x_1}\Delta x_1 + ... + \frac{\partial L}{\partial x_d}\Delta x_d = \nabla L ^T \Delta x&lt;/script&gt;. By now, things are becoming easier. Suppose, \(\Delta x=-\eta \nabla L\) (i.e. the step size in gradient descent, where \(\eta\) is the learning rate). Then:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla L \approx -\eta \nabla L^T\nabla L = -\eta||\nabla L||_2^2 \leq 0&lt;/script&gt;

&lt;p&gt;Now, we can find the rightness of gradient descent. We can use the following update rule to update next \(x\):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x^{k+1} = x^{k} - \eta \nabla L(x^k)&lt;/script&gt;

&lt;p&gt;this update rule will make the objective function drop to the minimum point.&lt;/p&gt;

&lt;h2 id=&quot;gradient-descent-in-a-convex-problem&quot;&gt;Gradient Descent in a convex problem&lt;/h2&gt;
&lt;p&gt;Now, I will consider the gradient descent in a convex problem, because we usually use gradient descent in a convex problem, otherwise, we usually get the local minimum. If the objective function is convex, then \(\nabla L(x^k)^T(x^{k+1}-x^{k})\geq 0\) implies \(L(x^{k+1}) \geq L(x^k)\). This can be derived from the convex property of a convex function, i.e. \(L(x^{k+1}) \geq L(x^k)^T(x^{k+1}-x^k)\).&lt;/p&gt;

&lt;p&gt;In this sense, we need to make \(\nabla L(x^k)^T(x^{k+1}-x^{k})\leq 0\) so as to make the objective function decrease. In gradient descent \(\Delta x\) is chosen to be \(-\nabla L(x^k)\). However, there are many other descent method, such as &lt;strong&gt;steepest descend&lt;/strong&gt;, &lt;strong&gt;normalized steepest descent&lt;/strong&gt;, &lt;strong&gt;newton step&lt;/strong&gt; and so on. The main idea of these methods is to make \(\nabla L(x^k)^T(x^{k+1}-x^{k})= \nabla L^T \Delta x \leq 0\).&lt;/p&gt;

&lt;h2 id=&quot;acknowledgement&quot;&gt;Acknowledgement&lt;/h2&gt;
&lt;p&gt;We would like to thank &lt;a href=&quot;https://github.com/Marvinmw&quot;&gt;Wei Ma&lt;/a&gt; and &lt;a href=&quot;https://github.com/yeephycho&quot;&gt;Yixuan Hu&lt;/a&gt; for checking the details of this blog.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Michael A. Nielsen, &lt;em&gt;Neural Networks and Deep Learning&lt;/em&gt;, Determination Press, 2015&lt;/li&gt;
  &lt;li&gt;Stephen Boyd, and Lieven Vandenberghe. &lt;em&gt;Convex optimization&lt;/em&gt;. Cambridge university press, 2004.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;remarks&quot;&gt;Remarks&lt;/h2&gt;
&lt;p&gt;Last updated on June 14, 2016&lt;/p&gt;

</description>
                <link>/mathGD</link>
                <guid>/gradientdescentmath</guid>
                <pubDate>Fri, 03 Jun 2016 00:00:00 +0200</pubDate>
        </item>

        <item>
                <title>Contributing an article</title>
                <description>&lt;p&gt;If you’re writing an article for this blog, please follow these guidelines.&lt;/p&gt;

&lt;p&gt;One of the rewards of switching my website to &lt;a href=&quot;http://jekyllrb.com/&quot;&gt;Jekyll&lt;/a&gt; is the 
ability to support &lt;strong&gt;MathJax&lt;/strong&gt;, which means I can write LaTeX-like equations that get 
nicely displayed in a web browser, like this one \( \sqrt{\frac{n!}{k!(n-k)!}} \) or 
this one \( x^2 + y^2 = r^2 \).&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;&lt;img class=&quot;centered&quot; src=&quot;http://gastonsanchez.com/images/blog/mathjax_logo.png&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;whats-mathjax&quot;&gt;What’s MathJax?&lt;/h3&gt;

&lt;p&gt;If you check MathJax website &lt;a href=&quot;http://www.mathjax.org/&quot;&gt;(www.mathjax.org)&lt;/a&gt; you’ll see 
that it &lt;em&gt;is an open source JavaScript display engine for mathematics that works in all 
browsers&lt;/em&gt;.&lt;/p&gt;

&lt;h3 id=&quot;how-to-implement-mathjax-with-jekyll&quot;&gt;How to implement MathJax with Jekyll&lt;/h3&gt;

&lt;p&gt;I followed the instructions described by Dason Kurkiewicz for 
&lt;a href=&quot;http://dasonk.github.io/blog/2012/10/09/Using-Jekyll-and-Mathjax/&quot;&gt;using Jekyll and Mathjax&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Here are some important details. I had to modify the Ruby library for Markdown in 
my &lt;code class=&quot;highlighter-rouge&quot;&gt;_config.yml&lt;/code&gt; file. Now I’m using redcarpet so the corresponding line in the 
configuration file is: &lt;code class=&quot;highlighter-rouge&quot;&gt;markdown: redcarpet&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;To load the MathJax javascript, I added the following lines in my layout &lt;code class=&quot;highlighter-rouge&quot;&gt;page.html&lt;/code&gt; 
(located in my folder &lt;code class=&quot;highlighter-rouge&quot;&gt;_layouts&lt;/code&gt;)&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;script&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;text/javascript&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;src&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;script&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Of course you can choose a different file location in your jekyll layouts.&lt;/p&gt;

&lt;h3 id=&quot;a-couple-of-examples&quot;&gt;A Couple of Examples&lt;/h3&gt;

&lt;p&gt;Here’s a short list of examples. To know more about the details behind MathJax, you can 
always checked the provided documentation available at 
&lt;a href=&quot;http://docs.mathjax.org/en/latest/&quot;&gt;http://docs.mathjax.org/en/latest/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I’m assuming you are familiar with LaTeX. However, you should know that MathJax does not 
have the exactly same behavior as LaTeX. By default, the &lt;strong&gt;tex2jax&lt;/strong&gt; preprocessor defines the 
LaTeX math delimiters, which are &lt;code class=&quot;highlighter-rouge&quot;&gt;\\(...\\)&lt;/code&gt; for in-line math, and &lt;code class=&quot;highlighter-rouge&quot;&gt;\\[...\\]&lt;/code&gt; for 
displayed equations. It also defines the TeX delimiters &lt;code class=&quot;highlighter-rouge&quot;&gt;$$...$$&lt;/code&gt; for displayed 
equations, but it does not define &lt;code class=&quot;highlighter-rouge&quot;&gt;$...$&lt;/code&gt; as in-line math delimiters. Fortunately, 
you can change these predefined specifications if you want to do so.&lt;/p&gt;

&lt;p&gt;Let’s try a first example. Here’s a dummy equation:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a^2 + b^2 = c^2&lt;/script&gt;

&lt;p&gt;How do you write such expression? Very simple: using &lt;strong&gt;double dollar&lt;/strong&gt; signs&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;o&quot;&gt;$$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;^&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;^&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;^&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$$&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;To display inline math use &lt;code class=&quot;highlighter-rouge&quot;&gt;\\( ... \\)&lt;/code&gt; like this &lt;code class=&quot;highlighter-rouge&quot;&gt;\\( sin(x^2) \\)&lt;/code&gt; which gets 
rendered as \( sin(x^2) \)&lt;/p&gt;

&lt;p&gt;Here’s another example using type &lt;code class=&quot;highlighter-rouge&quot;&gt;\mathsf&lt;/code&gt;&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;o&quot;&gt;$$&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;\&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mathsf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Data&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;PCs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;\&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;times&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;\&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mathsf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Loadings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$$&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;which gets displayed as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathsf{Data = PCs} \times \mathsf{Loadings}&lt;/script&gt;

&lt;p&gt;Or even better:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;err&quot;&gt;\\&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;\&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mathbf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;\&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mathbf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;\&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mathbf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;P&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;^&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;\&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mathsf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;\\&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;is displayed as&lt;/p&gt;

&lt;p&gt;\[ \mathbf{X} = \mathbf{Z} \mathbf{P^\mathsf{T}} \]&lt;/p&gt;

&lt;h2 id=&quot;important-notes&quot;&gt;Important notes&lt;/h2&gt;

&lt;h3 id=&quot;subscripts&quot;&gt;1. Subscripts&lt;/h3&gt;

&lt;p&gt;If you want to use subscripts like this \( \mathbf{X}_{n,p} \) you need to scape the 
underscores with a backslash like so &lt;code class=&quot;highlighter-rouge&quot;&gt;\mathbf{X}\_{n,p}&lt;/code&gt;:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;o&quot;&gt;$$&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;\&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mathbf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;\_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;\&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mathbf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;\_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;\&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mathbf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;\_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$$&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;will be displayed as&lt;/p&gt;

&lt;p&gt;\[ \mathbf{X}_{n,p} = \mathbf{A}_{n,k} \mathbf{B}_{k,p} \]&lt;/p&gt;

&lt;h3 id=&quot;vertical-line&quot;&gt;2. vertical line&lt;/h3&gt;

&lt;p&gt;If you want to use vertical line &lt;code class=&quot;highlighter-rouge&quot;&gt;|&lt;/code&gt;, you also need to scape the underscores with a blackslash like so &lt;code class=&quot;highlighter-rouge&quot;&gt;\|&lt;/code&gt;&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;o&quot;&gt;$$&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;\&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;\&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;\&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;\&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$$&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;will be displayed as&lt;/p&gt;

&lt;p&gt;\[ ||A|| \]&lt;/p&gt;

</description>
                <link>/contributing</link>
                <guid>/contribute</guid>
                <pubDate>Fri, 01 Jan 2010 00:00:00 +0100</pubDate>
        </item>


</channel>
</rss>
