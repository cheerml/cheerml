

 <!DOCTYPE HTML>
<html lang="en">
<head>
  <meta charset="UTF-8">
    
    
    
    
    <title>An Introduction to Word Embeddings - Part 1: Applications | Cheer ML</title>
    

    <meta name="viewport" content="width=device-width, initial-scale=1,user-scalable=no">
    
    <meta name="author" content="Jun Lu, Yixuan Hu">
    

    
    <!--<%- open_graph({twitter_id: theme.author.twitter, google_plus: theme.author.google_plus}) %>-->
    
    <meta name="description" content="page.description">
    
    <meta property="og:type" content="article">
    
    <meta property="og:title" content="An Introduction to Word Embeddings - Part 1: Applications">
    <meta property="og:url" content="/wordembedapp">
    <meta property="og:site_name" content="Cheer ML">
    <meta property="og:description" content="">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="An Introduction to Word Embeddings - Part 1: Applications">
    <meta name="twitter:description" content="page.description">
    <meta name="twitter:creator" content="@">
    <link rel="publisher" href="">

    
    <link rel="alternative" href="/atom.xml" title="Cheer ML" type="application/atom+xml">
    
    
    <link rel="icon" href="/assets/img/favicon.ico">
    
    
    <link rel="apple-touch-icon" href="/assets/img/jacman.jpg">
    <link rel="apple-touch-icon-precomposed" href="/assets/img/jacman.jpg">
    

    <link rel="stylesheet" href="/assets/css/style.css" type="text/css">
    <link rel="stylesheet" href="/assets/css/highlight.css" type="text/css">
</head>

  <body>
    <header>
        <div>
		    
			<div id="imglogo">
				<a href="/"><img src="/assets/img/logo.png" alt="Cheer ML" title="Cheer ML"/></a>
			</div>
			
			<div id="textlogo">
				<h1 class="site-name"><a href="/" title="Cheer ML">Cheer ML</a></h1>
				<!-- <h2 class="blog-motto">the essence of machine leaerning</h2> -->
				<h2 class="blog-motto">the essence of ma<font color="black">ch</font>ine l<font color="black">e</font>a<font color="black">er</font>ning</h2>
			</div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="menu">
			</a></div>
			<nav class="animated">
				<ul>
					<ul>
					 
						<li><a href="/">Home</a></li>
					
						<li><a href="/archives">Archives</a></li>
					
						<li><a href="/about">About</a></li>
					
					<li>
 					
					<form class="search" action="//google.com/search" method="get" accept-charset="utf-8">
						<label>Search</label>
						<input type="search" id="search" name="q" autocomplete="off" maxlength="20" placeholder="Search" />
						<input type="hidden" name="q" value="site:">
					</form>
					
					</li>
				</ul>
			</nav>	
</div>
    </header>
    <div id="container">
      



<div id="main" class="post" itemscope itemprop="blogPost">
  
	<article itemprop="articleBody"> 
	<header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/wordembedapp" title="An Introduction to Word Embeddings - Part 1: Applications" itemprop="url">An Introduction to Word Embeddings - Part 1: Applications</a>
  </h1>
  <p class="article-author">By
    
		<a href="https://www.linkedin.com/in/geelon/" title="Jun Lu, Yixuan Hu" target="_blank" itemprop="author">Aaron Geelon So</a>
		
  <p class="article-time">
    <time datetime="2017-12-14 00:00:00 +0100" itemprop="datePublished"> Published 2017-12-14</time>
    
  </p>
</header>
	<div class="article-content">
		
		<div id="toc" class="toc-article toc-content" style="display: none;">
		
			<!--<%- toc(item.content) %>-->
		
		</div>
		
		<p>If you already have a solid understanding of word embeddings and are well into your data science career, skip ahead to the <a href="/wordembedtheory">next part</a>!</p>

<p>Human language is <a href="https://en.wikipedia.org/wiki/The_Unreasonable_Effectiveness_of_Mathematics_in_the_Natural_Sciences">unreasonably effective</a> at describing how we relate to the world. With a few, short words, we can convey many ideas and actions with little ambiguity. Well, <a href="http://mentalfloss.com/article/24445/10-amelia-bedelia-isms">mostly</a>.</p>

<p>Because we’re capable of seeing and describing so much complexity, a lot of structure is implicitly encoded into our language. It is no easy task for a computer (or a human, for that matter) to learn natural language, for it entails understanding how we humans observe the world, if not understanding how to observe the world.</p>

<p>For the most part, computers can’t understand natural language. Our programs are still line-by-line instructions telling a computer what to do — they often miss nuance and context. How can you explain sarcasm to a machine?</p>

<p>There’s good news though. There’s been some important breakthroughs in natural language processing (NLP), the domain where researchers try to teach computers human language.</p>

<p>Famously, in 2013 Google researchers (Mikolov 2013) found a method that enabled a computer to learn relations between words such as:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">king</span><span class="o">-</span><span class="n">man</span><span class="o">+</span><span class="n">woman</span><span class="err">≈</span><span class="n">queen</span><span class="o">.</span>
</code></pre>
</div>

<p>This method, called word embeddings, has a lot of promise; it might even be able to reveal hidden structure in the world we see. Consider one relation it <a href="http://byterot.blogspot.ch/2015/06/five-crazy-abstractions-my-deep-learning-word2doc-model-just-did-NLP-gensim.html">discovered</a>:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">president</span><span class="o">-</span><span class="n">power</span><span class="err">≈</span><span class="n">prime</span> <span class="n">minister</span>
</code></pre>
</div>

<p>Admittedly, this might be one of those specious relations.</p>

<p>Joking aside, it’s worth studying word embeddings for at least two reasons. First, there are a lot of applications made possible by word embeddings. Second, we can learn from the way researchers approached the problem of deciphering natural language for machines.</p>

<p>In Part 1 of this article series, let’s take a look at the first of these reasons.</p>

<h2 id="uses-of-word-embeddings">Uses of Word Embeddings</h2>

<p>There’s no obvious way to usefully compare two words unless we already know what they mean. The goal of word-embedding algorithms is, therefore, <strong>to embed words with meaning based on their similarity or relationship with other words</strong>.</p>

<p>In practice, words are embedded into a real vector space, which comes with notions of distance and angle. We hope that these notions extend to the embedded words in meaningful ways, quantifying relations or similarity between different words. And empirically, they actually do!</p>

<p>For example, the Google algorithm I mentioned above discovered certain nouns are singular/plural or have gender (Mikolov 2013abc):</p>

<p><img src="/assets/blog/wordembedapp/relations-Copy.png" alt="img1" /></p>

<p>They also found a country-capital relationship:</p>

<p><img src="/assets/blog/wordembedapp/country-Copy.png" alt="img2" /></p>

<p>And as further evidence that a word’s meaning can be implied from its relationships with other words, they actually found that the learned structure for one language often correlated to that of another language, perhaps suggesting the possibility for <a href="https://en.wikipedia.org/wiki/Machine_translation">machine translation</a> through word embeddings (Mikolov 2013c):</p>

<p><img src="/assets/blog/wordembedapp/mt-Copy.png" alt="img3" /></p>

<p>They released their C code as the <a href="https://code.google.com/archive/p/word2vec/">word2vec</a> package, and soon after, others adapted the algorithm for more programming languages. Notably, for <a href="https://radimrehurek.com/gensim/index.html">gensim</a> (Python) and <a href="https://deeplearning4j.org/word2vec">deeplearning4j</a> (Java).</p>

<p>Today, many companies and data scientists have found different ways to incorporate word2vec into their businesses and research. <a href="https://www.slideshare.net/eshvk/spotifys-music-recommendations-lambda-architecture">Spotify</a> uses it to help provide music recommendation. <a href="http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/">Stitch Fix</a> uses it to recommend clothing. Google is thought to use word2vec in <a href="https://searchengineland.com/faq-all-about-the-new-google-rankbrain-algorithm-234440">RankBrain</a> as part of their search algorithm.</p>

<p>Other researchers are using <a href="https://niksto.com/rankbrain/">word2vec</a> for sentiment analysis, which attempts to identify the emotionality behind the words people use to communicate. For example, one <a href="https://arxiv.org/pdf/1606.02820.pdf">Stanford research group</a> looked at how the same words in different Reddit communities take on different connotations. Here’s an example with the word soft:</p>

<p><img src="/assets/blog/wordembedapp/reddit-Copy.png" alt="img4" /></p>

<p>As you can see, the word “soft” has a negative connotation when you’re talking about sports (you might think of the term “soft players”) while they have a positive connotation when you’re talking about cartoons.</p>

<p>And here are more examples where the computer could analyze the emotional sentiment of the same words across different communities.</p>

<p><img src="/assets/blog/wordembedapp/reddit-spectrum-Copy.png" alt="img5" /></p>

<p>They can even apply the same method over time, following how the word terrific, which meant horrific for the majority of the 20th century, has come to essentially mean great today.</p>

<p><img src="/assets/blog/wordembedapp/terrific-Copy.png" alt="img6" /></p>

<p>As a light-hearted example, one <a href="http://www.pelleg.org/shared/hp/download/fun-facts-wsdm.pdf">research group</a> used word2vec to help them determine whether a fact is surprising or not, so that they could automatically generate trivia facts.</p>

<p>The successes of word2vec have also helped spur on other forms of word embedding—<a href="https://arxiv.org/pdf/1506.02761.pdf">WordRank</a>, Stanford’s <a href="https://nlp.stanford.edu/projects/glove/">GloVe</a>, and Facebook’s <a href="https://research.fb.com/projects/fasttext/">fastText</a>, to name a few major ones.</p>

<p>These algorithms seek to improve on word2vec — they also look at texts through different units: characters, subwords, words, phrases, sentences, documents, and perhaps even units of thought. As a result, they allows us to think about not just word similarity, but also sentence similarity and  document similarity—like this paper did (Kusner 2015):</p>

<p><img src="/assets/blog/wordembedapp/wmd-Copy.png" alt="img6" /></p>

<p>Word embeddings <strong>transform human language meaningfully into a form conducive to numerical analysis</strong>. In doing so, they allow computers to explore the wealth of knowledge encoded implicitly into our own ways of speaking. <strong>We’ve barely scratched the surface of that potential</strong>.</p>

<p>Any individual programmer or scholar can use these tools and contribute new knowledge. Many areas of research and industry that could benefit from NLP have yet to be explored. Word embeddings and neural language models are powerful techniques. But perhaps the most powerful aspect of machine learning is its collaborative culture. Many, if not most, of the state-of-the-art methods are open-source, along with their accompanying research.</p>

<p>So, it’s there, if we want to take advantage. Now, the main obstacle is just ourselves. And maybe an expensive GPU.</p>

<p>For the theory behind word embeddings, see <a href="/wordembedtheory">Part 2</a>.</p>

<h2 id="reference">Reference</h2>

<ul>
  <li>(Hamilton 2016) Hamilton, William L., et al. “Inducing domain-specific sentiment lexicons from unlabeled corpora.” arXiv preprint arXiv:1606.02820 (2016).</li>
  <li>(Kusner 2015) Kusner, Matt, et al. “From word embeddings to document distances.” International Conference on Machine Learning. 2015.</li>
  <li>(Mikolov 2013a) Mikolov, Tomas, Wen-tau Yih, and Geoffrey Zweig. “Linguistic regularities in continuous space word representations.” hlt-Naacl. Vol. 13. 2013.</li>
  <li>(Mikolov 2013b) Mikolov, Tomas, et al. “Efficient estimation of word representations in vector space.” arXiv preprint arXiv:1301.3781 (2013).</li>
  <li>(Mikolov 2013c) Mikolov, Tomas, et al. “Distributed representations of words and phrases and their compositionality.” Advances in neural information processing systems. 2013.</li>
  <li>(Mikolov 2013d) Mikolov, Tomas, Quoc V. Le, and Ilya Sutskever. “Exploiting similarities among languages for machine translation.” arXiv preprint arXiv:1309.4168 (2013).</li>
</ul>

<h2 id="remark">Remark</h2>

<p>This blog content is requested by <a href="https://www.linkedin.com/in/gautambay/">Gautam Tambay</a> and edited by <a href="https://github.com/junlulocky">Jun Lu</a>. You can also find this blog at <a href="https://www.springboard.com/blog/introduction-word-embeddings/">Springboard</a>.</p>

  
	</div>
	<footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <!--
  <%- list_categories(item.categories, {
      show_count: false,
      class: 'article-category',
      style: 'none',
      separator: '►'
  }) %>
  -->
  
  <a class="article-category-link" href="/categories/#NLP">NLP</a>
  
  <a class="article-category-link" href="/categories/#Machine Learning">Machine Learning</a>
  
</div>


  <div class="article-tags">
  <!--
  <% var tags = [];
    item.tags.forEach(function(tag){
      tags.push('<a href="' + config.root + tag.path + '">' + tag.name + '</a>');
    }); %>-->
  <span></span> <!--<%- tags.join('') %>-->
  
  
  <a href="/tags/#Machine Learning">Machine Learning</a>
  
  </div>

</div>



	<div class="article-share" id="share">
	
	  <div data-url="http://localhost:4000/wordembedapp" data-title="An Introduction to Word Embeddings - Part 1: Applications | Cheer ML" data-tsina="" class="share clearfix">
	  </div>
	
	</div>


</footer>   
	</article>
	
<nav class="article-nav clearfix">
 
 <div class="prev" >
 <a href="/random-projection" title="Dimensionality Reduction via JL Lemma and Random Projection">
  <strong>Prev: </strong><br/>
  <span>
  Dimensionality Reduction via JL Lemma and Random Projection</span>
</a>
</div>


<div class="next">
<a href="/wordembedtheory"  title="An Introduction to Word Embeddings - Part 2: Problems and Theory">
 <strong>Next: </strong><br/> 
 <span>An Introduction to Word Embeddings - Part 2: Problems and Theory
</span>
</a>
</div>

</nav>

	

</div>  

      
      
      <div class="openaside"><a class="navbutton" href="#" title="Show Sidebar"></a></div>

  <div id="toc" class="toc-aside toc-content">
 
 <!--<%- toc(item.content) %>-->
 
  </div>

<div id="asidepart">
<div class="closeaside"><a class="closebutton" href="#" title="Hide Sidebar"></a></div>
<aside class="clearfix">


  
<div class="categorieslist">
	<p class="asidetitle">Categories</p>
		<ul>
		
		  
			<li><a href="/categories/#Announcement" title="Announcement">Announcement<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/#Machine Learning" title="Machine Learning">Machine Learning<sup>7</sup></a></li>
		  
		
		  
			<li><a href="/categories/#Deep Learning" title="Deep Learning">Deep Learning<sup>2</sup></a></li>
		  
		
		  
			<li><a href="/categories/#Theory" title="Theory">Theory<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/#NLP" title="NLP">NLP<sup>2</sup></a></li>
		  
		
		</ul>
</div>


  
<div class="tagslist">
	<p class="asidetitle">Tags</p>
		<ul class="clearfix">
		
			
				<li><a href="/tags/#resources" title="resources">resources<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/#jekyll" title="jekyll">jekyll<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/#Learning theory" title="Learning theory">Learning theory<sup>3</sup></a></li>
			
		
			
				<li><a href="/tags/#Deep Learning" title="Deep Learning">Deep Learning<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/#Systems" title="Systems">Systems<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/#Theory" title="Theory">Theory<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/#Machine Learning" title="Machine Learning">Machine Learning<sup>2</sup></a></li>
			
		
		</ul>
</div>


  <div class="linkslist">
  <p class="asidetitle">Links</p>
    <ul>
        
          <li>
            <a href="http://www.junlulocky.com" target="_blank" title="Jun Lu's website">Jun Lu's website</a>
          </li>
        
          <li>
            <a href="http://yeephycho.github.io/" target="_blank" title="Yixuan Hu's website">Yixuan Hu's website</a>
          </li>
        
          <li>
            <a href="https://github.com/IamTao" target="_blank" title="Tao Lin's website">Tao Lin's website</a>
          </li>
        
          <li>
            <a href="https://ovss.github.io" target="_blank" title="Junxiong Wang's website">Junxiong Wang's website</a>
          </li>
        
          <li>
            <a href="https://www.linkedin.com/in/geelon" target="_blank" title="Aaron Geelon So's website">Aaron Geelon So's website</a>
          </li>
        

    </ul>
</div>

  


  <div class="rsspart">
	<a href="/atom.xml" target="_blank" title="rss">RSS</a>
</div>


</aside>
</div>
    </div>
    <footer><div id="footer" >
	
	<div class="line">
		<span></span>
		<div class="author"></div>
	</div>
	
	
	<section class="info">
		<p> Hello, we are machine learning lovers. <br/>
			This is our blog, believe it or not.</p>
	</section>
	 
	<div class="social-font" class="clearfix">
		
		
		
		
		
		
		
		
		
		
	</div>
	<!--
			<%  Array.prototype.S=String.fromCharCode(2);
			  Array.prototype.in_array=function(e){
    			var r=new RegExp(this.S+e+this.S);
    			return (r.test(this.S+this.join(this.S)+this.S));
				};
				var cc = new Array('by','by-nc','by-nc-nd','by-nc-sa','by-nd','by-sa','zero'); %>
		<% if (cc.in_array(theme.creative_commons) ) { %>
				<div class="cc-license">
          <a href="http://creativecommons.org/licenses/<%= theme.creative_commons %>/4.0" class="cc-opacity" target="_blank">
            <img src="<%- config.root %>img/cc-<%= theme.creative_commons %>.svg" alt="Creative Commons" />
          </a>
        </div>
    <% } %>
				-->

		<p class="copyright">
		Powered by <a href="http://jekyllrb.com" target="_blank" title="jekyll">jekyll</a> and Theme by <a href="#" target="_blank" title="Jacman">Jacman</a> © 2017
		
		<a href="about" target="_blank" title="Jun Lu, Yixuan Hu">Jun Lu, Yixuan Hu</a>
		
		
		</p>
</div>
</footer>
    <script src="/assets/js/jquery-2.0.3.min.js"></script>
<script src="/assets/js/jquery.imagesloaded.min.js"></script>
<script src="/assets/js/gallery.js"></script>
<script src="/assets/js/jquery.qrcode-0.12.0.min.js"></script>
<script src="/assets/js/toc.js"></script>

<script type="text/javascript">
$(document).ready(function(){ 
  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      o = $('.openaside');
  c.click(function(){
    a.addClass('fadeOut').css('display', 'none');
    o.css('display', 'block').addClass('fadeIn');
    m.addClass('moveMain');
  });
  o.click(function(){
    o.css('display', 'none').removeClass('beforeFadeIn');
    a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');      
    m.removeClass('moveMain');
  });
  $(window).scroll(function(){
    o.css("top",Math.max(80,260-$(this).scrollTop()));
  });
  
  
  
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else{
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
      o.css('display', 'none');
      
      $('#toc.toc-aside').css('display', 'none');
      
    }
  });
});
</script>


<script type="text/javascript">
$(document).ready(function(){ 
  $('#toc.toc-aside').toc({
    title: "Contents",
    showEffect: "none"
  });
  $('#toc.toc-article').toc({
    title: "Contents",
    showEffect: "show",
    showSpeed: 0
  });
});
</script>



<script type="text/javascript">
$(document).ready(function(){ 
  var ai = $('.article-content>iframe'),
      ae = $('.article-content>embed'),
      t  = $('#toc'),
      ta = $('#toc.toc-aside'),
      o  = $('.openaside'),
      c  = $('.closeaside');
  if(ai.length>0){
    ai.wrap('<div class="video-container" />');
  };
  if(ae.length>0){
   ae.wrap('<div class="video-container" />');
  };
  c.click(function(){
    ta.css('display', 'block').addClass('fadeIn');
  });
  o.click(function(){
    ta.css('display', 'none');
  });
  $(window).scroll(function(){
    ta.css("top",Math.max(140,320-$(this).scrollTop()));
  });
});
</script>



<script type="text/javascript">
$(document).ready(function(){ 
  var $this = $('.share'),
      url = $this.attr('data-url'),
      encodedUrl = encodeURIComponent(url),
      title = $this.attr('data-title'),
      tsina = $this.attr('data-tsina'),
      description = $this.attr('description');
  var html = [
  '<div class="hoverqrcode clearfix"></div>',
  '<a class="overlay" id="qrcode"></a>',
  '<div class="qrcode clearfix"><span>扫描二维码分享到微信朋友圈</span><a class="qrclose" href="#nothing"></a><strong>Loading...Please wait</strong><img id="qrcode-pic" data-src="http://s.jiathis.com/qrcode.php?url=' + encodedUrl + '"/></div>',
  '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="article-share-facebook" target="_blank" title="Facebook"></a>',
  '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="article-share-twitter" target="_blank" title="Twitter"></a>',
  '<a href="#qrcode" class="article-share-qrcode" title="微信"></a>',
  '<a href="http://widget.renren.com/dialog/share?resourceUrl=' + encodedUrl + '&srcUrl=' + encodedUrl + '&title=' + title +'" class="article-share-renren" target="_blank" title="人人"></a>',
  '<a href="http://service.weibo.com/share/share.php?title='+title+'&url='+encodedUrl +'&ralateUid='+ tsina +'&searchPic=true&style=number' +'" class="article-share-weibo" target="_blank" title="微博"></a>',
  '<span title="Share to"></span>'
  ].join('');
  $this.append(html);
  
  $('.article-share-qrcode').click(function(){
    var imgSrc = $('#qrcode-pic').attr('data-src');
    $('#qrcode-pic').attr('src', imgSrc);
    $('#qrcode-pic').load(function(){
        $('.qrcode strong').text(' ');
    });
  });
/*
  $('.hoverqrcode').hide();

  var myWidth = 0;
  function updatehoverqrcode(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
    var qrsize = myWidth > 1024 ? 200:100;
    var options = {render: 'image', size: qrsize, fill: '#2ca6cb', text: url, radius: 0.5, quiet: 1};
    var p = $('.article-share-qrcode').position();
    $('.hoverqrcode').empty().css('width', qrsize).css('height', qrsize)
                          .css('left', p.left-qrsize/2+20).css('top', p.top-qrsize-10)
                          .qrcode(options);
  };
  $(window).resize(function(){
    $('.hoverqrcode').hide();
  });
  $('.article-share-qrcode').click(function(){
    updatehoverqrcode();
    $('.hoverqrcode').toggle();
  });
  $('.article-share-qrcode').hover(function(){}, function(){
      //$('.hoverqrcode').hide();
  });
  */
});   
</script>





<!--

-->




<link rel="stylesheet" href="/assets/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/assets/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  $('.article-content').each(function(i){
    $(this).find('img').each(function(){
      if ($(this).parent().hasClass('fancybox')) return;
      if ($(this).hasClass('emoji')) return;
      var alt = this.alt;
      if (alt) $(this).after('<span class="caption">' + alt + '</span>');
      $(this).wrap('<a href="' + this.src + '" title="' + alt + '" class="fancybox"></a>');
    });
    $(this).find('.fancybox').each(function(){
      $(this).attr('rel', 'article' + i);
    });
  });
  if($.fancybox){
    $('.fancybox').fancybox();
  }
}); 
</script>


<!-- Analytics Begin -->





<!-- Analytics End -->

<!-- Totop Begin -->

	<div id="totop">
	<a title="Back to Top"><img src="/assets/img/scrollup.png"/></a>
	</div>
	<script src="/assets/js/totop.js"></script>

<!-- Totop End -->

<!-- MathJax Begin -->


<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\[","\]"], ["\\[","\\]"] ],
      processEscapes: true
    },
    messageStyle: "none",
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<!--<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js">
</script> 

<!-- MathJax End -->

<!-- Tiny_search Begin -->

<!-- Tiny_search End -->
  </body>
</html>


